import os
from dotenv import load_dotenv
from src.CorpusSingleton import CorpusSingleton
from src.RecuperationDocs import RedditScrap, ArxivScrap
from src.GestionErreurs import GestionErreurs
from src.constantes import *
"""
@file rev1.py
@brief Point d'entr√©e principal du moteur de recherche documentaire.

@details
Ce fichier orchestre l'ex√©cution du scraping Reddit et Arxiv afin de constituer un corpus de documents.

Classes et Modules Utilis√©s :
- CorpusSingleton : Gestion du corpus unique de documents (pattern Singleton).
- RedditScrap : Scraping de la plateforme Reddit pour r√©cup√©rer des posts.
- ArxivScrap : Scraping de la plateforme Arxiv pour r√©cup√©rer des articles scientifiques.
- GestionErreurs : Gestion centralis√©e des erreurs et √©criture dans un fichier log.

Ex√©cution :
    python rev1.py

Sortie :
- Affiche la taille du corpus final et les documents extraits depuis Reddit et Arxiv.
- Les erreurs rencontr√©es sont logg√©es dans "app_errors.log".
"""

if __name__ == "__main__":
    """
    @brief Point d'entr√©e de l'application.

    @details
    Ce bloc initialise les composants principaux de l'application :
    - Initialisation de la gestion des erreurs.
    - Scraping de Reddit et Arxiv.
    - Affichage du corpus final.
    """
    
    # Charger les variables d'environnement
    load_dotenv()
    
    erreur = GestionErreurs(log_file="app_errors.log")
    
    theme = 'water' # mettre le th√®me souhait√© 
    # Chemin de sauvegarde dans le dossier 'data'
    dossier_data = "../data"
    theme_sans_espace= theme.replace(" ", "")
    nom_corpus = f"corpus_{theme_sans_espace}"
    nom_fichier = nom_corpus + ".pkl"
    chemin_sauvegarde = os.path.join(dossier_data, nom_fichier)
    print(f" {nom_corpus} {nom_fichier} {chemin_sauvegarde}")
    
    corpus = CorpusSingleton(nom_corpus=nom_corpus)

    # Cr√©er le dossier 'data' s'il n'existe pas
    if not os.path.exists(dossier_data):
        os.makedirs(dossier_data)
    
    # Charger ou cr√©er un nouveau corpus
    if os.path.exists(chemin_sauvegarde):
        print("üìÇ Chargement du corpus existant...")
        corpus = corpus.load(chemin_sauvegarde)
    else:
        print("üìÅ Aucune sauvegarde n'est trouv√©e, t√©l√©chargement √† nouveau")
                
        # Scraping Reddit et Arxiv
        erreur = GestionErreurs(log_file="app_errors.log")
        reddit_scraper = RedditScrap(corpus, erreur)
        reddit_scraper.recuperer_posts(theme, limit=10)

        arxiv_scraper = ArxivScrap(corpus, erreur)
        arxiv_scraper.recuperer_articles(theme, max_results=10)

        # Sauvegarder le corpus
        corpus.save(chemin_sauvegarde)
        print(f"üíæ Corpus sauvegard√© dans {chemin_sauvegarde}")
        
    
    # Afficher les documents r√©cup√©r√©s
    print(f"Taille du corpus : {len(corpus.id2doc)} documents")
    for doc in corpus.id2doc:
        print(doc)

    