import pandas as pd
import pickle
from datetime import datetime
import re
from src.Author import Author
from src.Frequence import Frequence 
from src.Utils import Utils
import uuid  # Pour g√©n√©rer des identifiants uniques


class Corpus:
    def __init__(self, nom_corpus=""):
        self.nom_corpus = nom_corpus
        self.authors = {}
        
        # dictionnaire d‚Äôindices id2doc dont les cl√©s sont les identifiants et les valeurs
        # les objets qui repr√©sentent les documents. {"id":"doc"}
        self.id2doc = {} # 
        self.ndoc = 0
        self.naut = 0
        self.texte_concatene = None  
        
    def ajouter_document(self, doc):
        """
        Ajoute un document au corpus avec nettoyage du texte.
        """        
        
        identifiant_unique = str(uuid.uuid4())  # G√©n√©ration d'un identifiant unique (UUID)
        
         # V√©rification de l'unicit√©
        if identifiant_unique in self.id2doc:
            print(f"‚ö†Ô∏è Document d√©j√† existant : {identifiant_unique}")
            return False
        
        # Ajouter le document avec l'ID g√©n√©r√©
        self.id2doc[identifiant_unique] = doc
        self.ndoc += 1

        # G√©rer l'auteur
        auteur_nom = doc.auteur
        if auteur_nom not in self.authors:
            self.authors[auteur_nom] = Author(auteur_nom)
            self.naut += 1
        
        self.authors[auteur_nom].add(identifiant_unique, doc)
        
        #print(f"‚úÖ Document ajout√© : {doc.titre} (ID: {identifiant_unique})")
        return True

    def search(self, mot_cle):
        """
        Recherche des passages contenant un mot-cl√© dans le texte concat√©n√© du corpus.
        """
        # V√©rification et concat√©nation automatique si n√©cessaire
        if not self.texte_concatene:
            print("üîÑ Concat√©nation automatique du corpus...")
            self.texte_concatene = Utils.concatener_textes(self)
        
        print(f"üîç Recherche du mot-cl√© : {mot_cle}\n")
        #print(f"le texte du corpus : \n {self.texte_concatene} \n" )

        # Regex : capturer 4 mots avant et apr√®s le mot-cl√©
        pattern = rf'(\b\w+(?:\s+\w+){{0,3}}\s+)\b{re.escape(mot_cle.lower())}\b(\s+\w+(?:\s+\w+){{0,3}})'

        
        passages = re.findall(pattern, self.texte_concatene)

        # Si aucun passage trouv√©
        if not passages:
            return ["Aucun r√©sultat trouv√©."]
        
        # Reconstituer les passages captur√©s
        resultats = [f"{p[0]}{mot_cle}{p[1]}" for p in passages]

        # Affichage pour contr√¥le
        for i,passage in enumerate(resultats):
            print(f"‚úÖ le passage {i + 1 } : {passage}\n")

        return resultats


    def concorde(self, mot_cle, taille_contexte=30):
        """
        Cr√©e un concordancier pour un mot-cl√© donn√© √† partir des textes du corpus.
        """
        # V√©rification et concat√©nation automatique si n√©cessaire
        if not self.texte_concatene:
            print("üîÑ Concat√©nation automatique du corpus...")
            self.texte_concatene = Utils.concatener_textes(self)
            
        occurences = list(re.finditer(re.escape(mot_cle), self.texte_concatene, re.IGNORECASE))

        contexte_avant = []
        mot_cle_trouve = []
        contexte_apres = []

        for occ in occurences:
            debut, fin = occ.start(), occ.end()
            mot_cle_trouve.append(self.texte_concatene[debut:fin])

            debut_contexte = max(0, debut - taille_contexte)
            contexte_avant.append(self.texte_concatene[debut_contexte:debut])

            fin_contexte = min(len(self.texte_concatene), fin + taille_contexte)
            contexte_apres.append(self.texte_concatene[fin:fin_contexte])

        df = pd.DataFrame({
            'contexte_avant': contexte_avant,
            'mot_cle_trouve': mot_cle_trouve,
            'contexte_apres': contexte_apres
        })
        return df
    
    def afficher_premiers_documents(self, n=5):
        """
        Affiche les n premiers documents du corpus.
        """
        print(f"\nüìÑ Affichage des {n} premiers documents du corpus :\n")
        for i, doc in enumerate(list(self.id2doc.values())[:n]):
            print(f"üîπ Document {i+1} :")
            print(f"   Titre : {doc.titre}")
            print(f"   Auteur : {doc.auteur}")
            print(f"   Date : {doc.date}")
            print(f"   URL : {doc.url}")
            print(f"   Contenu : {doc.texte}")  # Affiche seulement les 200 premiers caract√®res
            print("-" * 80)

    
    def stats(self, n=10):
        compteur = Frequence.compter_occurrences(self.id2doc.values())
        freq_df = pd.DataFrame(compteur.most_common(n), columns=['Mot', 'Fr√©quence'])
        return freq_df
    
    def trier_par_date(self, n=10):
        """
        @brief Trie les documents par date de publication.
        @param n Nombre de documents √† retourner (par d√©faut 10).
        @return Liste des n documents les plus r√©cents.
        """
        return sorted(self.id2doc.values(), key=lambda d: d.date, reverse=True)[:n]

    def trier_par_titre(self, n=10):
        """
        @brief Trie les documents par titre alphab√©tique.
        @param n Nombre de documents √† retourner (par d√©faut 10).
        @return Liste des n premiers documents tri√©s par titre.
        """
        return sorted(self.id2doc.values(), key=lambda d: d.titre)[:n]

    def __repr__(self):
        """
        @brief Retourne une repr√©sentation textuelle du corpus.
        @return Cha√Æne repr√©sentant le corpus (nom, nombre de documents et auteurs).
        """
        return f"Nom du corpus: {self.nom_corpus} - {self.ndoc} documents, {self.naut} auteurs , Texte : {self.texte_concatene}"

    def save(self, path):
        """
        @brief Retourne une repr√©sentation textuelle du corpus.
        @return Cha√Æne repr√©sentant le corpus (nom, nombre de documents et auteurs).
        """
        with open(path, 'wb') as f:
            pickle.dump(self, f)

    @staticmethod
    def load(path):
        """
        @brief Charge un corpus √† partir d'un fichier pickle.
        @param path Chemin du fichier de sauvegarde.
        @return Instance de Corpus charg√©e.
        """
        with open(path, 'rb') as f:
            return pickle.load(f)
    

if __name__ == "__main__":
    from src.Document import Document
    
    corpus = Corpus("water")
    corpus.ajouter_document(Document("Doc1", "Auteur1", datetime.now(), "url1", 
                                     "Monitoring water quality is essential for ecosystems, ensuring the survival of aquatic life and maintaining biodiversity. By preventing pollution and detecting harmful substances early, environmental agencies can protect rivers, lakes, and oceans, preserving delicate habitats and supporting local communities that rely on these ecosystems."))
    
    corpus.ajouter_document(Document("Doc2", "Auteur2", datetime.now(), "url2", 
                                     "The preservation of water resources helps prevent droughts by enhancing groundwater recharge and promoting efficient water usage. Conservation efforts, such as reforestation and wetland restoration, play a critical role in mitigating the impacts of climate change, safeguarding future water supplies for both human populations and natural environments."))
    
    corpus.ajouter_document(Document("Doc3", "Auteur3", datetime.now(), "url3", 
                                     "Water resources are vital for agriculture, providing essential irrigation to crops and sustaining livestock. Without sufficient water, food production can significantly decline, leading to shortages and economic instability in rural communities."))

    # test de search
    reustlat_search = corpus.search("water")
    print("R√©sultat search : \n", reustlat_search)
    
    # Test Concorde
    resultat_concorde = corpus.concorde("resources", taille_contexte=10)
    print("R√©sultat concorde : \n", resultat_concorde)
    
    # Statistiques des mots
    df = corpus.stats(10)
    print("Statistiques : ", df)
    
   
